<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->

<!-- Mirrored from keras.io/layers/core/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 13 Jun 2018 14:16:28 GMT -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Core Layers - Keras Documentation</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Source+Sans+Pro:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Core Layers";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','http://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-61785484-1', 'keras.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <a href="../../index.html"><div class="keras-logo">
        <img src="../../img/keras-logo-small-2018.jpg" class="keras-logo-img">
        Keras Documentation
      </div></a>
      <div class="wy-side-nav-search">
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="https://keras.io/search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../index.html">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../why-use-keras/index.html">Why use Keras</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Getting started</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../getting-started/sequential-model-guide/index.html">Guide to the Sequential model</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../getting-started/functional-api-guide/index.html">Guide to the Functional API</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../getting-started/faq/index.html">FAQ</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Models</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/about-keras-models/index.html">About Keras models</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/sequential/index.html">Sequential</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/model/index.html">Model (functional API)</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../about-keras-layers/index.html">About Keras layers</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="index.html">Core Layers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#dense">Dense</a></li>
                
            
                <li class="toctree-l3"><a href="#activation">Activation</a></li>
                
            
                <li class="toctree-l3"><a href="#dropout">Dropout</a></li>
                
            
                <li class="toctree-l3"><a href="#flatten">Flatten</a></li>
                
            
                <li class="toctree-l3"><a href="#input">Input</a></li>
                
            
                <li class="toctree-l3"><a href="#reshape">Reshape</a></li>
                
            
                <li class="toctree-l3"><a href="#permute">Permute</a></li>
                
            
                <li class="toctree-l3"><a href="#repeatvector">RepeatVector</a></li>
                
            
                <li class="toctree-l3"><a href="#lambda">Lambda</a></li>
                
            
                <li class="toctree-l3"><a href="#activityregularization">ActivityRegularization</a></li>
                
            
                <li class="toctree-l3"><a href="#masking">Masking</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../convolutional/index.html">Convolutional Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../pooling/index.html">Pooling Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../local/index.html">Locally-connected Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../recurrent/index.html">Recurrent Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../embeddings/index.html">Embedding Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../merge/index.html">Merge Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../advanced-activations/index.html">Advanced Activations Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../normalization/index.html">Normalization Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../noise/index.html">Noise layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../wrappers/index.html">Layer wrappers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../writing-your-own-keras-layers/index.html">Writing your own Keras layers</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Preprocessing</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/sequence/index.html">Sequence Preprocessing</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/text/index.html">Text Preprocessing</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/image/index.html">Image Preprocessing</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../losses/index.html">Losses</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../metrics/index.html">Metrics</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../optimizers/index.html">Optimizers</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../activations/index.html">Activations</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../callbacks/index.html">Callbacks</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../datasets/index.html">Datasets</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../applications/index.html">Applications</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../backend/index.html">Backend</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../initializers/index.html">Initializers</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../regularizers/index.html">Regularizers</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../constraints/index.html">Constraints</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../visualization/index.html">Visualization</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../scikit-learn-api/index.html">Scikit-learn API</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../utils/index.html">Utils</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributing/index.html">Contributing</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Keras Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Core Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="http://github.com/keras-team/keras" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L773">[source]</a></span></p>
<h3 id="dense">Dense</h3>
<pre><code class="python">keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>Just your regular densely-connected NN layer.</p>
<p><code>Dense</code> implements the operation:
<code>output = activation(dot(input, kernel) + bias)</code>
where <code>activation</code> is the element-wise activation function
passed as the <code>activation</code> argument, <code>kernel</code> is a weights matrix
created by the layer, and <code>bias</code> is a bias vector created by the layer
(only applicable if <code>use_bias</code> is <code>True</code>).</p>
<p>Note: if the input to the layer has a rank greater than 2, then
it is flattened prior to the initial dot product with <code>kernel</code>.</p>
<p><strong>Example</strong></p>
<pre><code class="python"># as first layer in a sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# now the model will take as input arrays of shape (*, 16)
# and output arrays of shape (*, 32)

# after the first layer, you don't need to specify
# the size of the input anymore:
model.add(Dense(32))
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>
<li><strong>activation</strong>: Activation function to use
    (see <a href="../../activations/index.html">activations</a>).
    If you don't specify anything, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix
    (see <a href="../../initializers/index.html">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="../../initializers/index.html">initializers</a>).</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="../../regularizers/index.html">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="../../regularizers/index.html">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="../../regularizers/index.html">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to
    the <code>kernel</code> weights matrix
    (see <a href="../../constraints/index.html">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="../../constraints/index.html">constraints</a>).</li>
</ul>
<p><strong>Input shape</strong></p>
<p>nD tensor with shape: <code>(batch_size, ..., input_dim)</code>.
The most common situation would be
a 2D input with shape <code>(batch_size, input_dim)</code>.</p>
<p><strong>Output shape</strong></p>
<p>nD tensor with shape: <code>(batch_size, ..., units)</code>.
For instance, for a 2D input with shape <code>(batch_size, input_dim)</code>,
the output would have shape <code>(batch_size, units)</code>.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L282">[source]</a></span></p>
<h3 id="activation">Activation</h3>
<pre><code class="python">keras.layers.Activation(activation)
</code></pre>

<p>Applies an activation function to an output.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>activation</strong>: name of activation function to use
    (see: <a href="../../activations/index.html">activations</a>),
    or alternatively, a Theano or TensorFlow operation.</li>
</ul>
<p><strong>Input shape</strong></p>
<p>Arbitrary. Use the keyword argument <code>input_shape</code>
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<p><strong>Output shape</strong></p>
<p>Same shape as input.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L80">[source]</a></span></p>
<h3 id="dropout">Dropout</h3>
<pre><code class="python">keras.layers.Dropout(rate, noise_shape=None, seed=None)
</code></pre>

<p>Applies Dropout to the input.</p>
<p>Dropout consists in randomly setting
a fraction <code>rate</code> of input units to 0 at each update during training time,
which helps prevent overfitting.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>rate</strong>: float between 0 and 1. Fraction of the input units to drop.</li>
<li><strong>noise_shape</strong>: 1D integer tensor representing the shape of the
    binary dropout mask that will be multiplied with the input.
    For instance, if your inputs have shape
    <code>(batch_size, timesteps, features)</code> and
    you want the dropout mask to be the same for all timesteps,
    you can use <code>noise_shape=(batch_size, 1, features)</code>.</li>
<li><strong>seed</strong>: A Python integer to use as random seed.</li>
</ul>
<p><strong>References</strong></p>
<ul>
<li><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
</ul>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L467">[source]</a></span></p>
<h3 id="flatten">Flatten</h3>
<pre><code class="python">keras.layers.Flatten(data_format=None)
</code></pre>

<p>Flattens the input. Does not affect the batch size.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>data_format</strong>: A string,
    one of <code>channels_last</code> (default) or <code>channels_first</code>.
    The ordering of the dimensions in the inputs.
    The purpose of this argument is to preserve weight
    ordering when switching a model from one data format
    to another.
    <code>channels_last</code> corresponds to inputs with shape
    <code>(batch, ..., channels)</code> while <code>channels_first</code> corresponds to
    inputs with shape <code>(batch, channels, ...)</code>.
    It defaults to the <code>image_data_format</code> value found in your
    Keras config file at <code>~/.keras/keras.json</code>.
    If you never set it, then it will be "channels_last".</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">model = Sequential()
model.add(Conv2D(64, 3, 3,
                 border_mode='same',
                 input_shape=(3, 32, 32)))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
</code></pre>

<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/engine/input_layer.py#L112">[source]</a></span></p>
<h3 id="input">Input</h3>
<pre><code class="python">keras.engine.input_layer.Input()
</code></pre>

<p><code>Input()</code> is used to instantiate a Keras tensor.</p>
<p>A Keras tensor is a tensor object from the underlying backend
(Theano, TensorFlow or CNTK), which we augment with certain
attributes that allow us to build a Keras model
just by knowing the inputs and outputs of the model.</p>
<p>For instance, if a, b and c are Keras tensors,
it becomes possible to do:
<code>model = Model(input=[a, b], output=c)</code></p>
<p>The added Keras attributes are:
<code>_keras_shape</code>: Integer shape tuple propagated
via Keras-side shape inference.
<code>_keras_history</code>: Last layer applied to the tensor.
the entire layer graph is retrievable from that layer,
recursively.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: A shape tuple (integer), not including the batch size.
    For instance, <code>shape=(32,)</code> indicates that the expected input
    will be batches of 32-dimensional vectors.</li>
<li><strong>batch_shape</strong>: A shape tuple (integer), including the batch size.
    For instance, <code>batch_shape=(10, 32)</code> indicates that
    the expected input will be batches of 10 32-dimensional vectors.
    <code>batch_shape=(None, 32)</code> indicates batches of an arbitrary number
    of 32-dimensional vectors.</li>
<li><strong>name</strong>: An optional name string for the layer.
    Should be unique in a model (do not reuse the same name twice).
    It will be autogenerated if it isn't provided.</li>
<li><strong>dtype</strong>: The data type expected by the input, as a string
    (<code>float32</code>, <code>float64</code>, <code>int32</code>...)</li>
<li><strong>sparse</strong>: A boolean specifying whether the placeholder
    to be created is sparse.</li>
<li><strong>tensor</strong>: Optional existing tensor to wrap into the <code>Input</code> layer.
    If set, the layer will not create a placeholder tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<p><strong>Example</strong></p>
<pre><code class="python"># this is a logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
</code></pre>

<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L316">[source]</a></span></p>
<h3 id="reshape">Reshape</h3>
<pre><code class="python">keras.layers.Reshape(target_shape)
</code></pre>

<p>Reshapes an output to a certain shape.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>target_shape</strong>: target shape. Tuple of integers.
    Does not include the batch axis.</li>
</ul>
<p><strong>Input shape</strong></p>
<p>Arbitrary, although all dimensions in the input shaped must be fixed.
Use the keyword argument <code>input_shape</code>
(tuple of integers, does not include the batch axis)
when using this layer as the first layer in a model.</p>
<p><strong>Output shape</strong></p>
<p><code>(batch_size,) + target_shape</code></p>
<p><strong>Example</strong></p>
<pre><code class="python"># as first layer in a Sequential model
model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
# now: model.output_shape == (None, 3, 4)
# note: `None` is the batch dimension

# as intermediate layer in a Sequential model
model.add(Reshape((6, 2)))
# now: model.output_shape == (None, 6, 2)

# also supports shape inference using `-1` as dimension
model.add(Reshape((-1, 2, 2)))
# now: model.output_shape == (None, 3, 2, 2)
</code></pre>

<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L415">[source]</a></span></p>
<h3 id="permute">Permute</h3>
<pre><code class="python">keras.layers.Permute(dims)
</code></pre>

<p>Permutes the dimensions of the input according to a given pattern.</p>
<p>Useful for e.g. connecting RNNs and convnets together.</p>
<p><strong>Example</strong></p>
<pre><code class="python">model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>dims</strong>: Tuple of integers. Permutation pattern, does not include the
    samples dimension. Indexing starts at 1.
    For instance, <code>(2, 1)</code> permutes the first and second dimension
    of the input.</li>
</ul>
<p><strong>Input shape</strong></p>
<p>Arbitrary. Use the keyword argument <code>input_shape</code>
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<p><strong>Output shape</strong></p>
<p>Same as the input shape, but with the dimensions re-ordered according
to the specified pattern.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L530">[source]</a></span></p>
<h3 id="repeatvector">RepeatVector</h3>
<pre><code class="python">keras.layers.RepeatVector(n)
</code></pre>

<p>Repeats the input n times.</p>
<p><strong>Example</strong></p>
<pre><code class="python">model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>n</strong>: integer, repetition factor.</li>
</ul>
<p><strong>Input shape</strong></p>
<p>2D tensor of shape <code>(num_samples, features)</code>.</p>
<p><strong>Output shape</strong></p>
<p>3D tensor of shape <code>(num_samples, n, features)</code>.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L572">[source]</a></span></p>
<h3 id="lambda">Lambda</h3>
<pre><code class="python">keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)
</code></pre>

<p>Wraps arbitrary expression as a <code>Layer</code> object.</p>
<p><strong>Examples</strong></p>
<pre><code class="python"># add a x -&gt; x^2 layer
model.add(Lambda(lambda x: x ** 2))
</code></pre>

<pre><code class="python"># add a layer that returns the concatenation
# of the positive part of the input and
# the opposite of the negative part

def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)

def antirectifier_output_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 2  # only valid for 2D tensors
    shape[-1] *= 2
    return tuple(shape)

model.add(Lambda(antirectifier,
                 output_shape=antirectifier_output_shape))
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>function</strong>: The function to be evaluated.
    Takes input tensor as first argument.</li>
<li><strong>output_shape</strong>: Expected output shape from function.
    Only relevant when using Theano.
    Can be a tuple or function.
    If a tuple, it only specifies the first dimension onward;
         sample dimension is assumed either the same as the input:
         <code>output_shape = (input_shape[0], ) + output_shape</code>
         or, the input is <code>None</code> and
         the sample dimension is also <code>None</code>:
         <code>output_shape = (None, ) + output_shape</code>
    If a function, it specifies the entire shape as a function of the
    input shape: <code>output_shape = f(input_shape)</code></li>
<li><strong>arguments</strong>: optional dictionary of keyword arguments to be passed
    to the function.</li>
</ul>
<p><strong>Input shape</strong></p>
<p>Arbitrary. Use the keyword argument input_shape
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<p><strong>Output shape</strong></p>
<p>Specified by <code>output_shape</code> argument
(or auto-inferred when using TensorFlow).</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L916">[source]</a></span></p>
<h3 id="activityregularization">ActivityRegularization</h3>
<pre><code class="python">keras.layers.ActivityRegularization(l1=0.0, l2=0.0)
</code></pre>

<p>Layer that applies an update to the cost function based input activity.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>l1</strong>: L1 regularization factor (positive float).</li>
<li><strong>l2</strong>: L2 regularization factor (positive float).</li>
</ul>
<p><strong>Input shape</strong></p>
<p>Arbitrary. Use the keyword argument <code>input_shape</code>
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<p><strong>Output shape</strong></p>
<p>Same shape as input.</p>
<hr />
<p><span style="float:right;"><a href="https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L29">[source]</a></span></p>
<h3 id="masking">Masking</h3>
<pre><code class="python">keras.layers.Masking(mask_value=0.0)
</code></pre>

<p>Masks a sequence by using a mask value to skip timesteps.</p>
<p>For each timestep in the input tensor (dimension #1 in the tensor),
if all values in the input tensor at that timestep
are equal to <code>mask_value</code>, then the timestep will be masked (skipped)
in all downstream layers (as long as they support masking).</p>
<p>If any downstream layer does not support masking yet receives such
an input mask, an exception will be raised.</p>
<p><strong>Example</strong></p>
<p>Consider a Numpy data array <code>x</code> of shape <code>(samples, timesteps, features)</code>,
to be fed to an LSTM layer.
You want to mask timestep #3 and #5 because you lack data for
these timesteps. You can:</p>
<ul>
<li>set <code>x[:, 3, :] = 0.</code> and <code>x[:, 5, :] = 0.</code></li>
<li>insert a <code>Masking</code> layer with <code>mask_value=0.</code> before the LSTM layer:</li>
</ul>
<pre><code class="python">model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(32))
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../convolutional/index.html" class="btn btn-neutral float-right" title="Convolutional Layers"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../about-keras-layers/index.html" class="btn btn-neutral" title="About Keras layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../about-keras-layers/index.html" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../convolutional/index.html" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>

<!-- Mirrored from keras.io/layers/core/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 13 Jun 2018 14:16:28 GMT -->
</html>
