<!DOCTYPE html>
<html lang="en">

<!-- Mirrored from blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 13 Jun 2018 14:17:38 GMT -->
<head>
        <title>A ten-minute introduction to sequence-to-sequence learning in Keras</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="theme/css/main.css" type="text/css" />
        <link rel="stylesheet" href="theme/css/pygment.css" type="text/css" />

        <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Source+Sans+Pro:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css">
        <link href="index.html" type="application/atom+xml" rel="alternate" title="The Keras Blog ATOM Feed" />


        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="//blog.keras.io/css/ie.css"/>
                <script src="//blog.keras.io/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="//blog.keras.io/css/ie6.css"/><![endif]-->

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1>
                    <a href="index-2.html">The Keras Blog </a>
                </h1>
                <p id="side">
                    <a href="https://github.com/fchollet/keras">Keras</a> is a Deep Learning library for Python, that is simple, modular, and extensible.
                </p>
                <nav><ul>
                <li><a href="index.html">Archives</a></li>
                    <li >
                        <a href="https://github.com/fchollet/keras">Github</a>
                    </li>
                    <li >
                        <a href="../keras.io/index.html">Documentation</a>
                    </li>
                    <li >
                        <a href="https://groups.google.com/forum/#!forum/keras-users">Google Group</a>
                    </li>
                </ul></nav>
        </header><!-- /#banner -->

<section id="content" class="body">
<article>
        <header> <h1 class="entry-title"><a href="a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
        rel="bookmark" title="Permalink to A ten-minute introduction to sequence-to-sequence learning in Keras">A ten-minute introduction to sequence-to-sequence learning in Keras</a></h1>  </header>
        <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-09-29T00:00:00+02:00">
                Fri 29 September 2017
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="https://twitter.com/fchollet">Francois Chollet</a>
        </address>
<p>In <a href="category/tutorials.html">Tutorials</a>. </p>
</p></footer><!-- /.post-info --><!-- /.post-info -->
        <p>I see this question a lot -- how to implement RNN sequence-to-sequence learning in Keras?
Here is a short introduction.</p>
<p>Note that this post assumes that you already have some experience
with recurrent networks and Keras.</p>
<hr />
<h2>What is sequence-to-sequence learning?</h2>
<p>Sequence-to-sequence learning (Seq2Seq) is about training models
to convert sequences from one domain (e.g. sentences in English) to sequences in another domain 
(e.g. the same sentences translated to French). </p>
<div class="highlight"><pre><span></span><span class="s">&quot;the cat sat on the mat&quot;</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">Seq2Seq</span> <span class="n">model</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="s">&quot;le chat etait assis sur le tapis&quot;</span>
</pre></div>


<p>This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) --
in general, it is applicable any time you need to generate text.</p>
<p>There are multiple ways to handle this task, either using RNNs or using 1D convnets. Here we will focus on RNNs.</p>
<h2>The trivial case: when input and output sequences have the same length</h2>
<p>When both input sequences and output sequences have the same length, you can implement such models simply with
a Keras LSTM or GRU layer (or stack thereof). This is the case in 
<a href="https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py">this example script</a> 
that shows how to teach a RNN to learn to add numbers, encoded as character strings:</p>
<p><img alt="Seq2seq inference" src="img/seq2seq/addition-rnn.png" /></p>
<p>One caveat of this approach is that it assumes that it is possible to generate <code>target[...t]</code> given <code>input[...t]</code>. That works in some cases (e.g. adding strings of digits) but does not work for most use cases. In the general case, information about the entire input sequence is necessary in order to start generating the target sequence.</p>
<h2>The general case: canonical sequence-to-sequence</h2>
<p>In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning "sequence to sequence models" with no further context. Here's how it works:</p>
<ul>
<li>A RNN layer (or stack thereof) acts as "encoder": it processes the input sequence and returns its own internal state.
    Note that we discard the outputs of the encoder RNN, only recovering the state.
    This state will serve as the "context", or "conditioning", of the decoder in the next step.</li>
<li>Another RNN layer (or stack thereof) acts as "decoder":
    it is trained to predict the next characters of the target sequence,
    given previous characters of the target sequence.
    Specifically, it is trained to turn the target sequences into
    the same sequences but offset by one timestep in the future,
    a training process called "teacher forcing" in this context.
    Importantly, the encoder uses as initial state the state vectors from the encoder,
    which is how the decoder obtains information about what it is supposed to generate.
    Effectively, the decoder learns to generate <code>targets[t+1...]</code>
    given <code>targets[...t]</code>, <em>conditioned on the input sequence</em>.</li>
</ul>
<p><img alt="Seq2seq inference" src="img/seq2seq/seq2seq-teacher-forcing.png" /></p>
<p>In inference mode, i.e. when we want to decode unknown input sequences, we go through a slightly different process:</p>
<ul>
<li>1) Encode the input sequence into state vectors.</li>
<li>2) Start with a target sequence of size 1
    (just the start-of-sequence character).</li>
<li>3) Feed the state vectors and 1-char target sequence
    to the decoder to produce predictions for the next character.</li>
<li>4) Sample the next character using these predictions
    (we simply use argmax).</li>
<li>5) Append the sampled character to the target sequence</li>
<li>6) Repeat until we generate the end-of-sequence character or we
    hit the character limit.</li>
</ul>
<p><img alt="Seq2seq inference" src="img/seq2seq/seq2seq-inference.png" /></p>
<p>The same process can also be used to train a Seq2Seq network <em>without</em> "teacher forcing", i.e. by reinjecting the decoder's predictions into the decoder.</p>
<h2>A Keras example</h2>
<p>Let's illustrate these ideas with actual code.</p>
<p>For our example implementation, we will use a dataset of pairs of English sentences and their French translation, which you can download from <a href="http://www.manythings.org/anki/">manythings.org/anki</a>. The file to download is called <code>fra-eng.zip</code>. We will implement a <em>character-level</em> sequence-to-sequence model, processing the input character-by-character and generating the output character-by-character. Another option would be a word-level model, which tends to be more common for machine translation. At the end of this post, you will find some notes about turning our model into a word-level model using <code>Embedding</code> layers.</p>
<p>The full script for our example <a href="https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py">can be found on GitHub</a>.</p>
<p>Here's a summary of our process:</p>
<ul>
<li>1) Turn the sentences into 3 Numpy arrays, <code>encoder_input_data</code>, <code>decoder_input_data</code>, <code>decoder_target_data</code>:<ul>
<li><code>encoder_input_data</code> is a 3D array of shape <code>(num_pairs, max_english_sentence_length, num_english_characters)</code>
    containing a one-hot vectorization of the English sentences.</li>
<li><code>decoder_input_data</code> is a 3D array of shape <code>(num_pairs, max_french_sentence_length, num_french_characters)</code>
    containg a one-hot vectorization of the French sentences.</li>
<li><code>decoder_target_data</code> is the same as <code>decoder_input_data</code> but <em>offset by one timestep</em>.
    <code>decoder_target_data[:, t, :]</code> will be the same as <code>decoder_input_data[:, t + 1, :]</code>.</li>
</ul>
</li>
<li>2) Train a basic LSTM-based Seq2Seq model to predict <code>decoder_target_data</code>
    given <code>encoder_input_data</code> and <code>decoder_input_data</code>.
    Our model uses teacher forcing.</li>
<li>3) Decode some sentences to check that the model is working (i.e. turn samples from <code>encoder_input_data</code>
    into corresponding samples from <code>decoder_target_data</code>).</li>
</ul>
<p>Because the training process and inference process (decoding sentences) are quite different, we use different
models for both, albeit they all leverage the same inner layers.</p>
<p>This is our training model. It leverages three key features of Keras RNNs:</p>
<ul>
<li>The <code>return_state</code> contructor argument, configuring a RNN layer to return a list where
    the first entry is the outputs and the next entries are the internal RNN states.
    This is used to recover the states of the encoder.</li>
<li>The <code>inital_state</code> call argument, specifying the initial state(s) of a RNN.
    This is used to pass the encoder states to the decoder as initial states.</li>
<li>The <code>return_sequences</code> constructor argument, configuring a RNN to return its full
    sequence of outputs (instead of just the last output, which the defaults behavior).
    This is used in the decoder.</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span>

<span class="c1"># Define an input sequence and process it.</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_encoder_tokens</span><span class="p">))</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="c1"># We discard `encoder_outputs` and only keep the states.</span>
<span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

<span class="c1"># Set up the decoder, using `encoder_states` as initial state.</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
<span class="c1"># We set up our decoder to return full output sequences,</span>
<span class="c1"># and to return internal states as well. We don&#39;t use the </span>
<span class="c1"># return states in the training model, but we will use them in inference.</span>
<span class="n">decoder_lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span>
                                     <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">)</span>
<span class="n">decoder_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>

<span class="c1"># Define the model that will turn</span>
<span class="c1"># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>
</pre></div>


<p>We train our model in two lines, while monitoring the loss on a held-out set of 20% of the samples.</p>
<div class="highlight"><pre><span></span><span class="c1"># Run training</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_data</span><span class="p">,</span> <span class="n">decoder_input_data</span><span class="p">],</span> <span class="n">decoder_target_data</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
          <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>


<p>After one hour or so on a MacBook CPU, we are ready for inference. To decode a test sentence, we will repeatedly:</p>
<ul>
<li>1) Encode the input sentence and retrieve the initial decoder state</li>
<li>2) Run one step of the decoder with this initial state and a "start of sequence" token as target. The output will be the next target character.</li>
<li>3) Append the target character predicted and repeat.</li>
</ul>
<p>Here's our inference setup:</p>
<div class="highlight"><pre><span></span><span class="n">encoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">)</span>

<span class="n">decoder_state_input_h</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<span class="n">decoder_state_input_c</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<span class="n">decoder_states_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">decoder_state_input_h</span><span class="p">,</span> <span class="n">decoder_state_input_c</span><span class="p">]</span>
<span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span>
    <span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">decoder_states_inputs</span><span class="p">)</span>
<span class="n">decoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>
<span class="n">decoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
    <span class="p">[</span><span class="n">decoder_inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states_inputs</span><span class="p">,</span>
    <span class="p">[</span><span class="n">decoder_outputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states</span><span class="p">)</span>
</pre></div>


<p>We use it to implement the inference loop described above:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decode_sequence</span><span class="p">(</span><span class="n">input_seq</span><span class="p">):</span>
    <span class="c1"># Encode the input as state vectors.</span>
    <span class="n">states_value</span> <span class="o">=</span> <span class="n">encoder_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>

    <span class="c1"># Generate empty target sequence of length 1.</span>
    <span class="n">target_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
    <span class="c1"># Populate the first character of target sequence with the start character.</span>
    <span class="n">target_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">target_token_index</span><span class="p">[</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">1.</span>

    <span class="c1"># Sampling loop for a batch of sequences</span>
    <span class="c1"># (to simplify, here we assume a batch of size 1).</span>
    <span class="n">stop_condition</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">decoded_sentence</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">stop_condition</span><span class="p">:</span>
        <span class="n">output_tokens</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">decoder_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="p">[</span><span class="n">target_seq</span><span class="p">]</span> <span class="o">+</span> <span class="n">states_value</span><span class="p">)</span>

        <span class="c1"># Sample a token</span>
        <span class="n">sampled_token_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">sampled_char</span> <span class="o">=</span> <span class="n">reverse_target_char_index</span><span class="p">[</span><span class="n">sampled_token_index</span><span class="p">]</span>
        <span class="n">decoded_sentence</span> <span class="o">+=</span> <span class="n">sampled_char</span>

        <span class="c1"># Exit condition: either hit max length</span>
        <span class="c1"># or find stop character.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">sampled_char</span> <span class="o">==</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="ow">or</span>
           <span class="nb">len</span><span class="p">(</span><span class="n">decoded_sentence</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_decoder_seq_length</span><span class="p">):</span>
            <span class="n">stop_condition</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="c1"># Update the target sequence (of length 1).</span>
        <span class="n">target_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
        <span class="n">target_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sampled_token_index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>

        <span class="c1"># Update states</span>
        <span class="n">states_value</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">decoded_sentence</span>
</pre></div>


<p>We get some nice results -- unsurprising since we are decoding samples taken from the training test.</p>
<div class="highlight"><pre><span></span>Input sentence: Be nice.
Decoded sentence: Soyez gentil !
-
Input sentence: Drop it!
Decoded sentence: Laissez tomber !
-
Input sentence: Get out!
Decoded sentence: Sortez !
</pre></div>


<p>This concludes our ten-minute introduction to sequence-to-sequence models in Keras. 
Reminder: the full code for this script <a href="https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py">can be found on GitHub</a>.</p>
<h2>References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
</ul>
<hr />
<h2>Bonus FAQ</h2>
<h3>What if I want to use a GRU layer instead of a LSTM?</h3>
<p>It's actually a bit simpler, because GRU has only one state, whereas LSTM has two states. Here's how to adapt the training model to use a GRU layer:</p>
<div class="highlight"><pre><span></span><span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_encoder_tokens</span><span class="p">))</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>

<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
<span class="n">decoder_gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_gru</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">state_h</span><span class="p">)</span>
<span class="n">decoder_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>
</pre></div>


<h3>What if I want to use a word-level model with integer sequences?</h3>
<p>What if your inputs are integer sequences (e.g. representing sequences of words, encoded by their index in a dictionary)? You can embed these integer tokens via an <code>Embedding</code> layer. Here's how:</p>
<div class="highlight"><pre><span></span><span class="c1"># Define an input sequence and process it.</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_encoder_tokens</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span>
                           <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

<span class="c1"># Set up the decoder, using `encoder_states` as initial state.</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)(</span><span class="n">decoder_inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Define the model that will turn</span>
<span class="c1"># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>

<span class="c1"># Compile &amp; run training</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">)</span>
<span class="c1"># Note that `decoder_target_data` needs to be one-hot encoded,</span>
<span class="c1"># rather than sequences of integers like `decoder_input_data`!</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_data</span><span class="p">,</span> <span class="n">decoder_input_data</span><span class="p">],</span> <span class="n">decoder_target_data</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
          <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>


<h3>What if I don't want to use teacher forcing for training?</h3>
<p>In some niche cases you may not be able to use teacher forcing, because you don't have access to the full target sequences, 
e.g. if you are doing online training on very long sequences, where buffering complete input-target pairs would be impossible. 
In that case, you may want to do training by reinjecting the decoder's predictions into the decoder's input, just like we were doing for inference.</p>
<p>You can achieve this by building a model that hard-codes the output reinjection loop: </p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Lambda</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="c1"># The first part is unchanged</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_encoder_tokens</span><span class="p">))</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

<span class="c1"># Set up the decoder, which will only process one timestep at a time.</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
<span class="n">decoder_lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">decoder_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>

<span class="n">all_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">decoder_inputs</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_decoder_seq_length</span><span class="p">):</span>
    <span class="c1"># Run the decoder on one timestep</span>
    <span class="n">outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                             <span class="n">initial_state</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="c1"># Store the current prediction (we will concatenate all predictions later)</span>
    <span class="n">all_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="c1"># Reinject the outputs as inputs for the next loop iteration</span>
    <span class="c1"># as well as update the states</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

<span class="c1"># Concatenate all predictions</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))(</span><span class="n">all_outputs</span><span class="p">)</span>

<span class="c1"># Define and compile model as previously</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">)</span>

<span class="c1"># Prepare decoder input data that just contains the start character</span>
<span class="c1"># Note that we could have made it a constant hard-coded in the model</span>
<span class="n">decoder_input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">))</span>
<span class="n">decoder_input_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">target_token_index</span><span class="p">[</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">1.</span>

<span class="c1"># Train model as previously</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">encoder_input_data</span><span class="p">,</span> <span class="n">decoder_input_data</span><span class="p">],</span> <span class="n">decoder_target_data</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
          <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>


<p>If you have more questions, please <a href="https://twitter.com/fchollet">reach out on Twitter</a>.</p>
        </div><!-- /.entry-content -->

</article>
</section>

        <footer id="footer" class="body">
                <address id="about" class="vcard body">
                Powered by <a href="http://alexis.notmyidea.org/pelican/">pelican</a>, which takes great advantages of <a href="http://python.org/">python</a>.
                </address><!-- /#about -->
        </footer><!-- /#footer -->

    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-61785484-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>

<!-- Mirrored from blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 13 Jun 2018 14:17:41 GMT -->
</html>